{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL DataBase Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib2\n",
    "import time\n",
    "import datetime as dt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## connect to database\n",
    "\n",
    "path = \"Data/mlb_data.db\"\n",
    "conn = sqlite3.connect(path)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fangraphs_woba(df, season, fg_dict, fg_df):\n",
    "    '''\n",
    "    To add Fangraphs' woba value to each event.\n",
    "    '''\n",
    "    for i in range(0, len(df)):\n",
    "        event = df.loc[[i], 'events'].values[0]\n",
    "    \n",
    "        if event in fg_dict.keys():\n",
    "            df.loc[i, 'fg_woba_value'] = fg_df.loc[fg_df[fg_df['Season'] == season].index, fg_dict[event]].values[0]\n",
    "        elif event == event:\n",
    "            df.loc[i, 'fg_woba_value'] = 0 \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New method - from website\n",
    "\n",
    "After browsing multiple scrapers on github, I chose to try to make my own. I decided to utilize the URL that Mr. Kessler used in his scraper (url below). I tried to make my own small loop scheme to import into a SQL database. I later realized it is similar to Mr. Kessler's. All credit for the link and method go to him and his scraper (namely, link, year/team loop idea, HTTPError catch and wait method).\n",
    "\n",
    "reference: https://github.com/alanrkessler/savantscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fangraphs wOBA Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, though, I grab the Fangraphs wOBA values. As these change anually, and Statcast's data includes Standard wOBA values (http://tangotiger.com/index.php/site/comments/standard-woba), we may prefer to use FG's. values. \n",
    "\n",
    "I scrape the website for the values and form a dataframe with them, to map using a dictionary to the baseball savant events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get fangraphs woba values\n",
    "\n",
    "link = 'https://www.fangraphs.com/guts.aspx?type=cn'\n",
    "\n",
    "page = requests.get(link)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "col_table = soup.find('thead')\n",
    "cols = col_table.find_all('th', class_ = 'rgHeader')\n",
    "\n",
    "df_columns = []\n",
    "\n",
    "for stat in cols:\n",
    "    df_columns.append(stat.text)\n",
    "    \n",
    "    \n",
    "stat_table = soup.find(class_ = 'rgMasterTable')\n",
    "stats1 = stat_table.find_all('tr', class_ = 'rgRow')\n",
    "stats2 = stat_table.find_all('tr', class_ = 'rgAltRow')\n",
    "\n",
    "stats = stats1 + stats2\n",
    "counter = 0\n",
    "\n",
    "for line in stats:\n",
    "    temp = []\n",
    "    \n",
    "    for i in range(1, 15):\n",
    "        temp.append(line.contents[i].text)\n",
    "    \n",
    "    temp = np.array(temp)\n",
    "    \n",
    "    if counter == 0:\n",
    "        fg_df = pd.DataFrame(temp.reshape(-1, len(temp)), columns = df_columns)\n",
    "        counter += 1\n",
    "    else:\n",
    "        fg_df = fg_df.append(pd.DataFrame(temp.reshape(-1, len(temp)), columns = df_columns))\n",
    "    \n",
    "fg_df = fg_df.sort_values('Season', ascending = False)\n",
    "fg_df = fg_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseball Savant Statcast\n",
    "\n",
    "I scrape the Baseball Savant search for data from each team, for each season listed. I append the data to tables in my SQL Database. I also add two columns to the data - spray angle of the hit (estimated using hit location) and Fangraphs wOBA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 Starting. Please wait (5 to 15 minutes, depending on length of season and connection speed)...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-53f3dd8c7933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfangraphs_woba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavant_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfg_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# user-defined function to add FG weighted woba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b8f83ddeab0c>\u001b[0m in \u001b[0;36mfangraphs_woba\u001b[0;34m(df, season, fg_dict, fg_df)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfg_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fg_woba_value'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Season'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mseason\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fg_woba_value'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "## year_list & team_list & FG to savant woba list\n",
    "\n",
    "year_list = [2015, 2016, 2017, 2018]\n",
    "\n",
    "team_list = ['SF', 'LAD', 'ARI', 'COL', 'SD',\n",
    "             'CHC', 'MIL', 'STL', 'CIN', 'PIT',\n",
    "             'NYM', 'WSH', 'MIA', 'ATL', 'PHI',\n",
    "             'OAK', 'HOU', 'LAA', 'TEX', 'SEA',\n",
    "             'MIN', 'CWS', 'KC', 'DET', 'CLE',\n",
    "             'NYY', 'BOS', 'TB', 'TOR', 'BAL']\n",
    "\n",
    "savant_dict = {'walk': 'wBB',\n",
    "              'single': 'w1B',\n",
    "              'double': 'w2B',\n",
    "              'triple': 'w3B',\n",
    "              'home_run': 'wHR',\n",
    "              'field_error': 'w1B',\n",
    "              'hit_by_pitch': 'w1B',\n",
    "              'catcher_interf': 'w1B'}\n",
    "\n",
    "\n",
    "## loop for each team and year\n",
    "\n",
    "for year in year_list:\n",
    "    \n",
    "    print(str(year) + ' Starting. Please wait' + \n",
    "          ' (5 to 15 minutes, depending on length of season and connection speed)...')\n",
    "\n",
    "    counter = 1      # if first team of year, replace existing table\n",
    "\n",
    "    for team in team_list:\n",
    "        done = False     # if done, stop trying to access link (stays false if error)\n",
    "                \n",
    "        while not done:\n",
    "            try:\n",
    "                ## non-nan link\n",
    "                link = 'https://baseballsavant.mlb.com/statcast_search/csv?all=true&hfPT=&hfAB=&hfBBT=&hfPR=' + \\\n",
    "                    '&hfZ=&stadium=&hfBBL=&hfNewZones=&hfGT=R%7C&hfC=&hfSea=' + str(year) + \\\n",
    "                    '%7C&hfSit=&player_type=pitcher&hfOuts=&opponent=&pitcher_throws=&batter_stands=&hfSA=' + \\\n",
    "                    '&game_date_gt=&game_date_lt=&team=' + team + \\\n",
    "                    '&position=&hfRO=&home_road=&hfFlag=&metric_1=&hfInn=&min_pitches=' + \\\n",
    "                    '0&min_results=0&group_by=name-event&sort_col=pitches&player_event_sort=' + \\\n",
    "                    'api_p_release_speed&sort_order=desc&min_abs=0&type=details&'\n",
    "                \n",
    "                ## nan-included link\n",
    "#                 link = 'https://baseballsavant.mlb.com/statcast_search/csv?all=true&hfPT=&hfAB=&hfBBT=&hfPR' + \\\n",
    "#                     '=&hfZ=&stadium=&hfBBL=&hfNewZones=&hfGT=R%7C&hfC=&hfSea=' + str(year) + \\\n",
    "#                     '%7C&hfSit=&player_type=pitcher&hfOuts=&opponent=&pitcher_throws=&batter_stands=&hfSA=' + \\\n",
    "#                     '&game_date_gt=&game_date_lt=&hfInfield=&team=' + team + \\\n",
    "#                     '&position=&hfOutfield=&hfRO=&home_road=&hfFlag=&hfPull=&metric_1=&hfInn=&min_pitches=' + \\\n",
    "#                     '0&min_results=0&group_by=name&sort_col=pitches&player_event_sort=h_launch_speed&' + \\\n",
    "#                     'sort_order=desc&min_pas=0&type=details&'\n",
    "                \n",
    "                \n",
    "                ## import data from link, a download csv link, and choose to replace table or append\n",
    "            \n",
    "                temp = pd.read_csv(link)\n",
    "                exists = ('replace' if counter == 1 else 'append') \n",
    "\n",
    "                \n",
    "                ## add columns - spray angle and fangraphs woba\n",
    "                \n",
    "                temp['spray_angle'] = \\\n",
    "    (np.arctan((temp['hc_x'] - 125.42)/(198.27 - temp['hc_y']))*180/np.pi*.75).apply(lambda x: round(x, 1))\n",
    "    \n",
    "                temp = temp.reset_index(drop = True)\n",
    "                temp = fangraphs_woba(temp, year, savant_dict, fg_df) # user-defined function to add FG weighted woba\n",
    "            \n",
    "            \n",
    "                ## import the data in the sql database and edit counter variables\n",
    "            \n",
    "                temp.to_sql(\"MLB_\" + str(year), conn, if_exists=exists, index = False)  # import to SQL\n",
    "                \n",
    "                done = True      # if import and link work, done\n",
    "                counter = counter + 1     # add to counter for each team completed\n",
    "                \n",
    "            except urllib2.HTTPError as e:     # catch an HTTP error if calling website too often\n",
    "                print(e)\n",
    "                print(str(year) + ' and ' + team + ' error...')\n",
    "                time.sleep(5)     # wait a minute before trying again\n",
    "        \n",
    "    print(str(year) + ' Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old method \n",
    "\n",
    "I manually downloaded csv files and stored them in a directory, by year. From there, I would import each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for year in range(2016, 2017):\n",
    "    \n",
    "\n",
    "#     ## current directory of data files\n",
    "\n",
    "#     cd = \"Data/savant/savant_\" + str(year) + \"/\"     # change on your machine\n",
    "\n",
    "\n",
    "#     ## create empty dataframe\n",
    "\n",
    "#     data = pd.DataFrame()\n",
    "\n",
    "\n",
    "#     ## loop through each sheet and append the previous one\n",
    "\n",
    "#     for file_name in os.listdir(cd):\n",
    "#         if 'DS' not in file_name:\n",
    "#             import_data = pd.read_csv(cd + file_name).replace(\n",
    "#                 'null', np.nan).convert_objects(convert_numeric = True)\n",
    "#             data = data.append(import_data)\n",
    "\n",
    "\n",
    "#     ## add spray angle\n",
    "\n",
    "#     data['spray_angle'] = (np.arctan((data['hc_x'] - 125.42)/(198.27 - data['hc_y']))*180/np.pi*.75).apply(lambda x: round(x, 1))\n",
    "\n",
    "\n",
    "#     # add dataframe to database\n",
    "\n",
    "#     data.to_sql(\"MLB_\" + str(year), conn, if_exists=\"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLB ID key\n",
    "\n",
    "To have a key to map names to numeric MLB player IDs.\n",
    "\n",
    "source: http://crunchtimebaseball.com/baseball_map.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## current directory of data files\n",
    "\n",
    "# cd = \"http://crunchtimebaseball.com/master.csv\"     # website of linked file\n",
    "\n",
    "\n",
    "# ## create empty dataframe\n",
    "\n",
    "# data = pd.read_csv(cd, encoding = 'latin-1').replace('null', np.nan).infer_objects()\n",
    "\n",
    "\n",
    "# # add dataframe to database\n",
    "\n",
    "# data.to_sql(\"ID_Key\", conn, if_exists=\"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Checks\n",
    "\n",
    "Check to see the tables listed to confirm their existance, and see the amount of data in each season table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(c.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for year in [2015, 2016, 2017, 2018]:\n",
    "    df = pd.read_sql(\"\"\"SELECT game_date\n",
    "        FROM MLB_{}\n",
    "        ;\"\"\".format(year), conn)\n",
    "    print year, len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## close access to database\n",
    "\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
