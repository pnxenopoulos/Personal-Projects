{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tim Boudreau <br>\n",
    "tim.boudreau25@gmail.com* <br>\n",
    "https://github.com/timboudreau25/\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hit Prediction\n",
    "\n",
    "\n",
    "Not every batted ball with quality contact becomes a hit. Likewise, neither does every poorly hit ball become an out. This project is to create a model that can predit whether or not a batted ball is a hit, based on numerous factors. A model like this, done right, can aid in the valuation of players. I believe tools like this can help discern true talent levels of players, discerning skill versus possible batted ball (mis)fortune.\n",
    "\n",
    "I chose to analyze batted balls to predict whether or not a single batted ball would be a hit. The algorithm I thought bets to use was the random forest algorithm, because it allows multiple iterations as well as is proficient for classification.\n",
    "\n",
    "\n",
    "*note, if images do not show up, either open this in the same directory as images, or open the PDF version*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Source:\n",
    "\n",
    "Most of my data is from BaseballSavant (https://baseballsavant.mlb.com/about) and its Statcast search tool. I formed a SQL database of every pitch thrown and their outcomes from the 2017 regular season.\n",
    "\n",
    "Each query from BaseballSavant returned approximately 30,000 pitches, which was about a week's worth of pitches. Appending these weeks took a while, so I created this database to swiftly import the data for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timb/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from IPython.display import Image\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## connect to database\n",
    "\n",
    "path = \"Data/mlb_data.db\"\n",
    "conn = sqlite3.connect(path)\n",
    "c = conn.cursor()\n",
    "\n",
    "\n",
    "## ignore knuckle balls, pitch outs, intentional balls, etc.\n",
    "\n",
    "\n",
    "## execute query for pitch data\n",
    "\n",
    "data_df = pd.read_sql(\"\"\"SELECT batter, events, release_speed, \n",
    "\trelease_spin_rate, pfx_x, pfx_z, plate_x, plate_z, \n",
    "    home_team, stand, spray_angle\n",
    "    p_throws, hc_x, hc_y, hit_distance_sc, launch_speed,\n",
    "    launch_angle, inning_topbot, away_team, estimated_woba_using_speedangle as woba\n",
    "\tFROM MLB_2017\n",
    "    WHERE events NOT IN ('sac_bunt')\n",
    "\t;\"\"\", conn).dropna()\n",
    "\n",
    "\n",
    "## query for mlb batter key and set it as a dictionary\n",
    "\n",
    "key = pd.read_sql(\"\"\"SELECT mlb_id, mlb_name FROM ID_Key;\"\"\", conn)\n",
    "c.close()\n",
    "\n",
    "key = key.set_index('mlb_id')['mlb_name'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected many variables for this analysis: pitch release spin and speed, pitch movement (in horizontal and vertical directions), pitch location crossing home plate, hit location, hit distance, launch angle and launch speed, ballpark, pitcher and batter handedness and fielding team. Some of these I created in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "\n",
    "I created some dummy variables and removed nulls from my data. After preparing the data, I created a copy of the dataset, as I drop the list of batters for the random forest regressions while I want to keep the batter list for batter analysis later.\n",
    "\n",
    "To prepare the data for analysis, first I removed the null contents. What was unique about BaseballSavant's dataset is that, instead of having a null cell, the datasets would have a string \"null\" instead. In creating my SQL databse, I removed the \"null\" strings and left them as null values (which sometimes appeared as NaN). After the query, I removed NaN values from the dataset. This did not significantly reduce the sample size.\n",
    "\n",
    "Beyond removing nulls, I created a boolean column classifying if a pitch resulted in a hit or not. Most of the null data I removed were null in hit metrics, as they were balls or strikes thrown. This was the leading factor in my decision to solely analyze balls in play. Of the remaining data, all were put in play. \n",
    "\n",
    "I also created a boolean for whether the inning of the pitch was the top or the bottom of the inning. I did this to create a fielding column, where the team fielding was a dummy in the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add a boolean column for whether or not the batted ball was a hit\n",
    "\n",
    "hits = ['single', 'double', 'triple', 'home_run']\n",
    "\n",
    "data_df['hits'] = (data_df['events'].isin(hits)).astype(int)\n",
    "\n",
    "\n",
    "## map the batter key to the dataset to replace batter index with names\n",
    "\n",
    "data_df['batter'] = data_df['batter'].map(key)\n",
    "\n",
    "\n",
    "## create a column for the fielding team\n",
    "\n",
    "def fielding_team(row):\n",
    "    if row['inning_topbot'] == 'Bot':       # if it's the bottom of the inning, the away team fields\n",
    "        return row['away_team']\n",
    "    elif row['inning_topbot'] == 'Top':     # if it's the top of the inning, the home team fields\n",
    "        return row['home_team']\n",
    "    else:\n",
    "        return\"failed\"\n",
    "\n",
    "\n",
    "data_df['fielding'] = data_df.apply(lambda row: fielding_team(row), axis = 1)   # apply formula from above\n",
    "\n",
    "\n",
    "## remove unecessary columns used in variable creation\n",
    "\n",
    "del data_df['away_team']\n",
    "del data_df['inning_topbot']\n",
    "del data_df['events']\n",
    "\n",
    "\n",
    "## rename columns we will create dummies from\n",
    "\n",
    "data_df = data_df.rename(columns = {'home_team' : 'park', 'stand' : 'batter_hand',\n",
    "                            'p_throws' : 'pitcher_hand'})\n",
    "\n",
    "\n",
    "## convert strings to boolean integers\n",
    "\n",
    "col_list = data_df.select_dtypes([np.object]).columns[1:]\n",
    "\n",
    "data_df = pd.get_dummies(data_df, prefix = col_list, columns = col_list)\n",
    "\n",
    "\n",
    "## create a copy dataframe, to later compare the outcomes of removing null outliers\n",
    "\n",
    "data = data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Outliers\n",
    "\n",
    "Considering outliers, I plotted the distribution of values for each numerical variable in the analysis. To save the space in this markdown, I plotted only the two variables who had outliers that I felt needed to be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## print a distribution of values for every numerical variable to scan for outliers\n",
    "\n",
    "# for col in data.columns:\n",
    "#     if data[col].dtype in (np.float64, np.int64):\n",
    "#         sns.distplot(data[col])\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "sns.distplot(data['hc_x'])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(data['hc_y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I discovered these outliers while plotting the hit locations in Tableau. The hit x and hit y coordinates both have noticable occurances at 0 and 240. I'm unsure of the units in this dataset - I'm unable to find a directory - but it was clear these points were mistakes, likely produced as nulls by the tools that acquired the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## remove the outliers - the max and min values seen in the distributions above\n",
    "\n",
    "data = data[(data['hc_y'] > 0) & (data['hc_y'] < 248)\n",
    "        & (data['hc_x'] > 0) & (data['hc_x'] < 248)]\n",
    "\n",
    "\n",
    "## print the percentage of batted balls that are hits with and without the removed values\n",
    "\n",
    "print \"Percent of batted balls as hits - original data: \", round(sum(data_df['hits']) / len(data_df), 3)\n",
    "print \"Percent of batted balls as hits - after removing outliers: \", round(sum(data['hits']) / len(data), 3), \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing unexpected outliers, the percentage of batted balls that were hit remained essentially unchanged. This implies that the outliers has no significant affect on the distribution of hits. I still chose to remove them, though, to clean the data further. The lack of change likely ould be due to the small amount of broken outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Model Preparation\n",
    "\n",
    "To prepare the clean data for modeling, I ignored the batter list from the dataset I used to model. Rather than remove the batters directly, I copied the dataset and then removed the batters. Later, I want to use the batters in evaluating the model on a small sample. Beyond removing the batters, I created a hit key to label the boolean zeroes and ones that represent hits and outs. After building the model, I will use this key to map hits and outs for clarity in output display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables = data.columns[(data.columns != \"hits\") & (data.columns != \"woba\")][1:]\n",
    "\n",
    "hit_key = {}\n",
    "\n",
    "hit_key = pd.DataFrame( {'string' : ['hit', 'out'],\n",
    "                   'integer' : [1, 0]}).set_index('integer')['string'].T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a random forest classifier with two iterations\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, data[variables], data['hits'], cv=5)\n",
    "print \"Cross-validated scores:\", scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to run the random forest classifier with two iterations because of the time saved running two evrsus, say, ten and because the model didn't improve with more iterations. To properly build the model, I chose to cross-validate five times. As seen with the cross-validation scores, the model improved with further training. The score is an arbitrary mark, derived from the Mean Squared Error and parameter defining, that implies the quality of the model. A higher score implies a stronger model fit. Using cross validation to train the model increased the quality of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Modeling\n",
    "\n",
    "Below, I created a dataframe with the results of our prediction and the actual hit/out classification of each ball in play. I printed out a confusion matrix, which reflects on the correct and incorrect classifications by the model on the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'prediction' : cross_val_predict(clf, data[variables], data['hits'], cv=5),\n",
    "                     \"actual\" : data['hits']}).applymap(hit_key.get)\n",
    "\n",
    "pd.crosstab(results.actual, results.prediction)\n",
    "# .apply(lambda r: (r/r.sum() * 100).astype(float).round(6), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(results.actual, results.prediction).apply(lambda r: (r/r.sum() * 100).astype(float).round(4), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first confusion matrix is the numerical amount of correct and incorrect predictions. The seocnd confusion matrix is the percent of correct and incorrect classifications. For example, in these confusion matrices, the top-right quadrant is the count (or percentge) of actual hits the model predicted as outs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['prediction'] = (results['prediction'] == \"hit\").astype(int)\n",
    "\n",
    "\n",
    "print \"\\nPercent of batted balls as hits - predicted hits: \", round(sum(data['prediction']) / len(data), 3)\n",
    "print \"Percent of batted balls as hits - original data: \", round(sum(data['hits']) / len(data), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, the data's percentage of batted balls that are hits is 35.4%. The predicted percentage, however, is lower, close to 31.2% (may slightly change if kernel isn't set and program is rerun). Prior to removing data points with null hit distances, the original percentage was closer to 33%. Further investigation is needed in why there is a discrepency between the model and actual, but I believe this could be due to the lack of fielder position data, which significantly impacts hit outcomes, or it could be the model struggling to depict, in the infield, fielder-induced hits, such as poor routes to balls, or well-placed balls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same(row):\n",
    "    if row['hits'] == row['prediction']:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "data['correct'] = data.apply(same, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a column called \"correct\" that is true when the prediction was correct and false when it was incorrect. I used this for plotting in Tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batter Analysis\n",
    "\n",
    "I thought it would be interesting to apply this model to batters. Being able to predict a batter's potential hit count is my goal. I don't expect the results to be significant, though, as the sample sizes are a bit small (below 500 occurances per batter). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "To analyze hit prediction for each batter, I had to group the data by batter. To enhance the quality of my analysis, I limited the batters to those with more than 100 batted balls. I stored prediction values and actual values for hits in a dataframe, and use a formula to detemrine whether it is a false positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## group the data by batter and create a count variable to limit sample to batters with more than 100 batted balls\n",
    "\n",
    "data['count'] = data.groupby('batter')['batter'].transform('count')\n",
    "\n",
    "batter_data = data[data['count'] > 100]\n",
    "\n",
    "\n",
    "## create a list of batters. commented out is a short list for quick use\n",
    "\n",
    "batter_list = batter_data['batter'].unique()\n",
    "# batter_list = ['Brandon Crawford', \"Buster Posey\", \"Denard Span\"]\n",
    "\n",
    "\n",
    "## define the columns in the dataframe that will collect batter prediction results\n",
    "\n",
    "cols = [['batter', 'false positives', 'false negatives', 'correct']]\n",
    "\n",
    "result_list = pd.DataFrame([])\n",
    "\n",
    "\n",
    "## define a function that classifies a wrong prediction as a false negative or positive\n",
    "\n",
    "def false_check(row):\n",
    "    if (row['prediction'] is \"hit\") & (row['actual'] is \"out\"):   \n",
    "        return \"False Positive\"\n",
    "    elif (row['actual'] is \"hit\") & (row['prediction'] is \"out\"): \n",
    "        return \"False Negative\"\n",
    "    else:\n",
    "        return \"Correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop through the batters took longer than expected, close to 10 minutes on my machine. For sake of speed, I created a list of three batter that I use to test the code, but I ran through the entire list of batters for this markdown. The for loop runs through each batter, breaks the dataframe down into their batted balls and predicts for each batted ball if it is a hit. I then stored those values in a dataframe, label the 1's as hits and 0's as outs, create false positives and negatives and repeat for every batter. I also created a percentage correct/false positive/negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## loop through every batter in our list to calculate predicted hits and the errors\n",
    "\n",
    "for batter in batter_list:\n",
    "    \n",
    "    \n",
    "    ## create a subset of data where each batted ball is by our batter\n",
    "    \n",
    "    data_batter = data[data['batter'] == batter]\n",
    "\n",
    "    \n",
    "    ## predict the batted ball hit outcomes and store it alongside actual hit outcomes,\n",
    "    ## mapping hit and out to 1 and 0\n",
    "    \n",
    "    batter_results = pd.DataFrame({'prediction' : cross_val_predict(clf, data_batter[variables], data_batter['hits'], cv=5),\n",
    "                     \"actual\" : data_batter['hits']}).applymap(hit_key.get)\n",
    "\n",
    "\n",
    "    ## calling the above function, classify each prediction by type of error or correct\n",
    "    \n",
    "    batter_results['type'] = batter_results.apply(lambda row: false_check(row), axis = 1)   # apply formula from above\n",
    "\n",
    "    \n",
    "    ## count the total amount of errors by type, and correct\n",
    "    \n",
    "    false_positive = sum(batter_results['type'].str.count(\"False Positive\"))\n",
    "    false_negative = sum(batter_results['type'].str.count(\"False Negative\"))\n",
    "    correct = sum(batter_results['type'].str.count(\"Correct\"))\n",
    "    \n",
    "    \n",
    "    ## store the error counts and correct count in a dataframe, by batter\n",
    "    \n",
    "    result_list = result_list.append(pd.DataFrame({ \"batter\" : batter,\n",
    "                                                    \"false_positive\" : false_positive,\n",
    "                                                    \"false_negative\" : false_negative,\n",
    "                                                    \"correct\" : correct}, index = [0]), ignore_index = True)\n",
    "\n",
    "\n",
    "## find the pcerentage of each error and correct for each batter\n",
    "\n",
    "result_list['percent_correct'] = result_list['correct']/result_list.sum(axis = 1)\n",
    "result_list['percent_false_pos'] = result_list['false_positive']/result_list.sum(axis = 1)\n",
    "result_list['percent_false_neg'] = result_list['false_negative']/result_list.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## print the top 8 correct and bottom 8 incorrect predicted batters\n",
    "\n",
    "print result_list.nlargest(8, 'percent_correct')[['batter', 'percent_correct']]\n",
    "print \"\\n\", result_list.nsmallest(8, 'percent_correct')[['batter', 'percent_correct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't been able to determine a characteristic or multiple that impact batters being on the top prediction list or bottom. My initial theory was that slower batters would be predicted better, as they have a much lower chance of getting an infield hit (which, as seen later, is one area where the model struggles). Another theory I had was that extreme pull or push hitters, who are easier to shift on, would be better predicted. Browsing the top 8 lists, though, nothing stood out. There are slow batters on the bottom as well as a few pull hitters. This is something I want to further research, with batter-specific data like speed scores, spray charts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## export data to csv to use in Tableau\n",
    "\n",
    "data.to_csv(\"prediction.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I exported the data to a csv file to grachically display in Tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation & Results\n",
    "\n",
    "Below are graphs evaluating the performance of the model. In summary, I believe the model is accurate in many areas and struggles where one would expect it to struggle, such as in the infield with weaker contact.\n",
    "\n",
    "I plotted these graphs in Tableau. The dark blue points are hits and the light blue points are outs. Red points are false positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<center><table><tr><td><img src='Launch Speed vs Launch Angle, Actual.png' alt='Drawing' style='width: 1000px;'/></td><td><img src='Launch Speed vs Launch Angle, Predictions.png' alt='Drawing' style='width: 1000px;'/></td></tr></table></center>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graphic plots launch speed against launch angle. As seen here and in many datasets, there are areas where the batted balls have an extremely high probability of hits. Comparing the plots of actual hit classifications to the predicted hit classifications reveals areas in which the model performed poorly - namely, low launch angle, with both low and high exit velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<center><td><img src='Launch Speed vs Launch Angle, Actual wOBA.png' alt='Drawing' style='width: 500px;'/></td></tr></center>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this chart to the ones above it. This is a chart of exit velocity versus launch angle, like above, but colored by weighted on-base-average (wOBA). The darker the blue, the higher wOBA, with orange being a below league-average wOBA.\n",
    "\n",
    "As a refresher, wOBA is an all-encompassing offensive measure used to evaluate overall offensive performance per plate appearance, using linear weights on all potential batter outcomes. It is similar to batting average, on base percentage, slugging percentage and OPS as it tries to quantitatively value outcomes of at bats, however wOBA weighs each outcome based on the impact on run scoring (the run value of certain events). The league average wOBA is .320, where an excellent wOBA is anything above .400. The dark blue area, for context, is reaches a wOBA of 1.985! \n",
    "\n",
    "Notice that the actual and predicted hits fall in the same exit velocity - launch angle belt as high wOBA values. The model does well at predicting hits, which directly impact wOBA. The model struggles where factors outside our model, such as batter speed and fielder location, impact the outcome of a hit. This model is a pitch-based and batted-ball-based model, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<center><table><tr><td><img src='Launch Speed vs Launch Angle, False Positives.png' alt='Drawing' style='width: 1000px;'/></td><td><img src='Launch Speed vs Launch Angle, False Negatives.png' alt='Drawing' style='width: 1000px;'/></td></tr></table></center>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots are of where the model incorrectly classified batted balls. They both have the same axes as above, launch distance versus launch angle. On the left is where there were false positives i.e. the model classified a hit where it was an out, while on the right are false negatives, where an out was classified as a hit.\n",
    "\n",
    "Most of the false positives were along the periphery of the hit belt seen above, which is to be expected. Almost every batted ball with those launch angles and exit velocities was a hit.\n",
    "\n",
    "Many of the false negatives were in two general areas - as weak ground balls, where the hit classifications are influenced by fielding and batter speed, and as hits along the hit belt. The sheer amount of batted balls along the hit belt could be why there are many misclassifications there. Beyond that, batted balls along the edge of the hit belt may fall for hits on occasion, but are more similar to outs than hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<center><table><tr><td><img src='Actual Hits and Outs, Field View.png' alt='Drawing' style='width: 1000px;'/></td><td><img src='Predicted Hits and Outs, Field View.png' alt='Drawing' style='width: 1000px;'/></td></tr></table></center>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batted ball is plotted, with dark points as hits (or projected hits) and lighter points as outs. It's clear where there are hits, as they are where one would expect hits (and predicted hits) to be. Howver, there is a significant areas of misclassification that stands out - the infield.\n",
    "\n",
    "Infield batted balls are almost completely projected to be outs, while there are a noticable amount of infield hits in actual. This is the main driver behind the inaccuracy of the model. Knowing batter speed and fielder positioning would help increase this accuracy. Batter speed is important because many quick runners can beat out softly-hit grounders. Fielder position is relevant as well because it isn't currently known where the fielders are located for each batted ball. If there is a large shift on a left handed batter, for example, a soft dribbler down the third base line may be an easy hit, but this model would assume it to be a routine grounder to the third baseman.\n",
    "\n",
    "Gap hits were an issue until I calculated and included the spray angle of the batted ball. That inclusion increased gap hit/out prediction greatly, though there still are areas in the gaps that are noticably incorrect. Gap hits are difficult to predict because, again, fielder positioning is unknown. A typical outfield shift places an outfielder (or two!) in the middle of the gaps. Knowing those fielding locations would allow for more descriptive regression models in this random forest to predict batted ball outcomes. Beyond fielding location data, individual fielder data would potentially allow for accuracy increases in prediction. Fielder speeds, route efficiencies and initial starting positions, whether encompassed in defensive runs saved (DRS) or ultimate zone rating (UZR) or as individual metrics, would further explain outcomes of batted balls and increase predictive capabilities of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hits are simultaneously easy and difficult to predict. Given pitch characteristics and batted ball information, my model predicted 79%+ of the hits and 95%+ of outs correctly (up from 75% and 94% without including spray angle). The gaps in actual versus predicted could be shrunk with fielder data, such as positioning, speed, route efficiencies, etc., and with batter information, such as batter speed. As seen in the graphics, the majority of the false predictions were in two general locations, where deeper fielder and batter information could greatly impact the model's performance. Without that information, it is difficult to characterize why some batters were predicted well and others poorly. \n",
    "\n",
    "\n",
    "### Next Steps\n",
    "- Include batter information and fielder information, such as positioning and shifts, in the model.\n",
    "- Gather batter statistics to find trends in batter prediction abilities.\n",
    "- Consider removing gap and infield batted balls to evaluate efficiency elsewhere."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
