{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL DataBase Formation\n",
    "\n",
    "A Jupyter Notebook that should create and populate a sqlite db with Statcast data from Baseball Savant for any year listed by the user in the year array. This modified script should append to existing tables, as opposed to replace the table and rewrite it entirely (as the previous versiou did)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib2\n",
    "import time\n",
    "import datetime as dt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## connect to database\n",
    "\n",
    "path = \"Data/mlb_data.db\"\n",
    "conn = sqlite3.connect(path)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fangraphs_woba(df, season, fg_dict, fg_df):\n",
    "    '''\n",
    "    To add Fangraphs' woba value to each event.\n",
    "    '''\n",
    "    year_dict = {}\n",
    "    map_dict = {}\n",
    "\n",
    "    \n",
    "    ## for each column in fangraphs values, add to dictionary\n",
    "    \n",
    "    for col in fg_df[fg_df['Season'] == season].columns:\n",
    "        year_dict[col] = float(fg_df[fg_df['Season'] == season][col].values[0])\n",
    "\n",
    "        \n",
    "    ## for each value in savant to fg dictionary, grab fg value\n",
    "        \n",
    "    for k in fg_dict:\n",
    "        map_dict[k] = year_dict[fg_dict[k]]\n",
    "      \n",
    "    \n",
    "    ## for each remaining event not mapped, map\n",
    "    \n",
    "    for e in df['events'].unique():\n",
    "        if e not in fg_dict and e == e:\n",
    "            map_dict[e] = 0\n",
    "\n",
    "            \n",
    "    ## apply dictionary map to dataframe\n",
    "            \n",
    "    df['fg_woba_value'] = df['events'].map(map_dict)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_date(table, conn):\n",
    "    '''\n",
    "    Get last date of games imported\n",
    "    '''\n",
    "    table_check = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT name FROM sqlite_master WHERE name='{}'\n",
    "        \"\"\".format(table), conn)\n",
    "\n",
    "    if table_check.empty:\n",
    "        date = ''\n",
    "    else:\n",
    "        df = pd.read_sql(\n",
    "            \"\"\"\n",
    "            SELECT game_date\n",
    "            FROM {}\n",
    "            ORDER BY game_date DESC\n",
    "            LIMIT 1\n",
    "            ;\"\"\".format(table), conn)\n",
    "\n",
    "        date = (dt.datetime.strptime(df.values[0][0], '%Y-%m-%d') + \n",
    "                dt.timedelta(days = 1)).strftime('%Y-%m-%d')  \n",
    "        \n",
    "    return date\n",
    "\n",
    "\n",
    "\n",
    "def fill_db(teams, last, recent, year, savant, fg):\n",
    "    '''\n",
    "    Function to fill db for each year. Appending if possible, otherwise replace.\n",
    "    '''\n",
    "    for team in teams:\n",
    "        done = False     # if done, stop trying to access link (stays false if error)\n",
    "        \n",
    "        while not done:\n",
    "            try:\n",
    "                ## non-nan link\n",
    "                link = 'https://baseballsavant.mlb.com/statcast_search/csv?all=true&hfPT=&hfAB=&hfBBT=&hfPR=' + \\\n",
    "                    '&hfZ=&stadium=&hfBBL=&hfNewZones=&hfGT=R%7C&hfC=&hfSea=' + str(year) + \\\n",
    "                    '%7C&hfSit=&player_type=pitcher&hfOuts=&opponent=&pitcher_throws=&batter_stands=&hfSA=' + \\\n",
    "                    '&game_date_gt=' + last + \\\n",
    "                    '&game_date_lt=' + recent + \\\n",
    "                    '&team=' + team + \\\n",
    "                    '&position=&hfRO=&home_road=&hfFlag=&metric_1=&hfInn=&min_pitches=' + \\\n",
    "                    '0&min_results=0&group_by=name-event&sort_col=pitches&player_event_sort=' + \\\n",
    "                    'api_p_release_speed&sort_order=desc&min_abs=0&type=details&'\n",
    "                \n",
    "                ## nan-included link\n",
    "#                 link = 'https://baseballsavant.mlb.com/statcast_search/csv?all=true&hfPT=&hfAB=&hfBBT=&hfPR' + \\\n",
    "#                     '=&hfZ=&stadium=&hfBBL=&hfNewZones=&hfGT=R%7C&hfC=&hfSea=' + str(year) + \\\n",
    "#                     '%7C&hfSit=&player_type=pitcher&hfOuts=&opponent=&pitcher_throws=&batter_stands=&hfSA=' + \\\n",
    "#                     '&game_date_gt=&game_date_lt=&hfInfield=&team=' + team + \\\n",
    "#                     '&position=&hfOutfield=&hfRO=&home_road=&hfFlag=&hfPull=&metric_1=&hfInn=&min_pitches=' + \\\n",
    "#                     '0&min_results=0&group_by=name&sort_col=pitches&player_event_sort=h_launch_speed&' + \\\n",
    "#                     'sort_order=desc&min_pas=0&type=details&'\n",
    "                \n",
    "                \n",
    "                ## import data from link, a download csv link\n",
    "                temp = pd.read_csv(link)\n",
    "                                \n",
    "                ## add columns - spray angle and fangraphs woba\n",
    "                temp['spray_angle'] = \\\n",
    "                        (np.arctan((temp['hc_x'] - 125.42)/(198.27 - temp['hc_y'])) \\\n",
    "                         *180/np.pi*.75).apply(lambda x: round(x, 1))\n",
    "    \n",
    "                ## reset index and append Fangraphs wOBA weights to table\n",
    "                temp = temp.reset_index(drop = True)\n",
    "                temp = fangraphs_woba(temp, str(year), savant, fg) # user-defined function to add FG weighted woba\n",
    "            \n",
    "                ## import the data in the sql database\n",
    "                temp.to_sql(table, conn, if_exists='append', index = False)\n",
    "                \n",
    "                ## if import worked, finish loop\n",
    "                done = True\n",
    "                \n",
    "            except urllib2.HTTPError as e:     # catch an HTTP error if calling website too often\n",
    "                print(e)\n",
    "                print(str(year) + ' and ' + team + ' error...')\n",
    "                time.sleep(5)     # wait a minute before trying again\n",
    "        \n",
    "    if len(temp) == 0:\n",
    "        print(str(year) + ' already fully imported.')\n",
    "    else:\n",
    "        print(str(year) + ' Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New method - from website\n",
    "\n",
    "After browsing multiple scrapers on github, I chose to try to make my own. I decided to utilize the URL that Mr. Kessler used in his scraper (url below). I tried to make my own small loop scheme to import into a SQL database. I later realized it is similar to Mr. Kessler's. All credit for the link and method go to him and his scraper (namely, link, year/team loop idea, HTTPError catch and wait method).\n",
    "\n",
    "reference: https://github.com/alanrkessler/savantscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fangraphs wOBA Values\n",
    "\n",
    "First, though, I grab the Fangraphs wOBA values. As these change anually, and Statcast's data includes Standard wOBA values (http://tangotiger.com/index.php/site/comments/standard-woba), we may prefer to use FG's. values. \n",
    "\n",
    "I scrape the website for the values and form a dataframe with them, to map using a dictionary to the baseball savant events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get fangraphs woba values\n",
    "\n",
    "link = 'https://www.fangraphs.com/guts.aspx?type=cn'\n",
    "\n",
    "page = requests.get(link)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "col_table = soup.find('thead')\n",
    "cols = col_table.find_all('th', class_ = 'rgHeader')\n",
    "\n",
    "df_columns = []\n",
    "\n",
    "for stat in cols:\n",
    "    df_columns.append(stat.text)\n",
    "    \n",
    "    \n",
    "stat_table = soup.find(class_ = 'rgMasterTable')\n",
    "stats1 = stat_table.find_all('tr', class_ = 'rgRow')\n",
    "stats2 = stat_table.find_all('tr', class_ = 'rgAltRow')\n",
    "\n",
    "stats = stats1 + stats2\n",
    "counter = 0\n",
    "\n",
    "for line in stats:\n",
    "    temp = []\n",
    "    \n",
    "    for i in range(1, 15):\n",
    "        temp.append(line.contents[i].text)\n",
    "    \n",
    "    temp = np.array(temp)\n",
    "    \n",
    "    if counter == 0:\n",
    "        fg_df = pd.DataFrame(temp.reshape(-1, len(temp)), columns = df_columns)\n",
    "        counter += 1\n",
    "    else:\n",
    "        fg_df = fg_df.append(pd.DataFrame(temp.reshape(-1, len(temp)), columns = df_columns))\n",
    "    \n",
    "fg_df = fg_df.sort_values('Season', ascending = False)\n",
    "fg_df = fg_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseball Savant Statcast\n",
    "\n",
    "I scrape the Baseball Savant search for data from each team, for each season listed. I append the data to tables in my SQL Database. I also add two columns to the data - spray angle of the hit (estimated using hit location) and Fangraphs wOBA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2015 Starting. Please wait...\n",
      "2015 already fully imported.\n",
      "\n",
      "2016 Starting. Please wait...\n",
      "2016 already fully imported.\n",
      "\n",
      "2017 Starting. Please wait...\n",
      "2017 already fully imported.\n",
      "\n",
      "2018 Starting. Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timb/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "## year_list & team_list & FG to savant woba list\n",
    "\n",
    "year_list = [2015, 2016, 2017, 2018]\n",
    "\n",
    "team_list = ['SF', 'LAD', 'ARI', 'COL', 'SD',\n",
    "             'CHC', 'MIL', 'STL', 'CIN', 'PIT',\n",
    "             'NYM', 'WSH', 'MIA', 'ATL', 'PHI',\n",
    "             'OAK', 'HOU', 'LAA', 'TEX', 'SEA',\n",
    "             'MIN', 'CWS', 'KC', 'DET', 'CLE',\n",
    "             'NYY', 'BOS', 'TB', 'TOR', 'BAL']\n",
    "\n",
    "savant_dict = {'walk': 'wBB',\n",
    "              'single': 'w1B',\n",
    "              'double': 'w2B',\n",
    "              'triple': 'w3B',\n",
    "              'home_run': 'wHR',\n",
    "              'field_error': 'w1B',\n",
    "              'hit_by_pitch': 'w1B',\n",
    "              'catcher_interf': 'w1B'}\n",
    "\n",
    "\n",
    "## loop for each team and year\n",
    "\n",
    "for year in year_list:\n",
    "    \n",
    "    print('\\n' + str(year) + ' Starting. Please wait...')\n",
    "\n",
    "    ## table name to add to\n",
    "    table = 'MLB_' + str(year)\n",
    "    \n",
    "    \n",
    "    try:        \n",
    "        ## get yesterday's date and last date of import\n",
    "        yesterday_date = (dt.datetime.utcnow() - dt.timedelta(days = 1)).strftime('%Y-%m-%d') \n",
    "        last_date = get_date(table, conn)\n",
    "        \n",
    "        ## fill database\n",
    "        fill_db(team_list, last_date, yesterday_date, year, savant_dict, fg_df)\n",
    "    \n",
    "    \n",
    "    except sqlite3.OperationalError as e:\n",
    "        ## error means somehting is different with appending data than previous data in table\n",
    "        print('\\n  Error: ' + str(e) + '\\nReplacing table...\\n')\n",
    "        \n",
    "        ## replace whole table - remove it and reupload\n",
    "        c.execute(\"DROP TABLE \" + table)\n",
    "        last_date = ''\n",
    "        latest_date = ''\n",
    "        \n",
    "        ## fill database\n",
    "        fill_db(team_list, last_date, latest_date, year, savant_dict, fg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLB ID key\n",
    "\n",
    "To have a key to map names to numeric MLB player IDs.\n",
    "\n",
    "source: http://crunchtimebaseball.com/baseball_map.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## current directory of data files\n",
    "\n",
    "cd = \"http://crunchtimebaseball.com/master.csv\"     # website of linked file\n",
    "\n",
    "\n",
    "## create empty dataframe\n",
    "\n",
    "data = pd.read_csv(cd, encoding = 'latin-1').replace('null', np.nan).infer_objects()\n",
    "\n",
    "\n",
    "# add dataframe to database\n",
    "\n",
    "data.to_sql(\"ID_Key\", conn, if_exists=\"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Checks\n",
    "\n",
    "Check to see the tables listed to confirm their existance, and see the amount of data in each season table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(c.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for year in [2015, 2016, 2017, 2018]:\n",
    "    df = pd.read_sql(\"\"\"SELECT game_date\n",
    "        FROM MLB_{}\n",
    "        ;\"\"\".format(year), conn)\n",
    "    print year, len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## close access to database\n",
    "\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
